# MICAI_2023



This repository contains the resources used to write "Information retrieval techniques for Question Answering based on pre-trained language models" a comparative study between two prominent pre-trained language models, RoBERTa and GPT-3, focused on their performance in Question Answering (QA).
Our findings reveal insights into the strengths and limitations of each model, shedding light on their suitability for specific QA applications in the finance and legal domain.

For more details about our system, please refer to our article: "Information retrieval techniques for Question Answering based on pre-trained language models"
